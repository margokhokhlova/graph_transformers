{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "This demo shows the performance of other SOTA graph embeddings methods and their limitations:\n",
    "they do not take attributes into account (only can handle discreet attributes)\n",
    "they are very dependable onto the reinitialization of the random walks and minor graph changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "sys.path.append(os.path.realpath('lib'))\n",
    "from lib.data_loader import load_local_data\n",
    "from benchmarks.sub2vec import Sub2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub2vec demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach the authours were inspired by the paragraph2vec method of paragrahps embeddings. Creates sub-graph embeddings which takes neighborhood & structural information (separately) into account. The neighborhood info is similar to node2vec. The structural info is described using random walks, where each node is represented as a ratio of node degree to the size of  a subgraph, the graph-paths track how the density of edges changes in a neighbourhood. The nodes are coded (depending on their degree/graph size ratio) into a log scale vocabulary. To learn the final embeddings, researchers propose two methods: sub2vec-DM (predicting neigh node and graph) and sud2vec-DBON (co-occurence based learning). Negative samling is used as in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vects  2000\n",
      "(2000, 128)\n"
     ]
    }
   ],
   "source": [
    "dataset_n='aids' # the IGN  and BZR are the alternative datasets\n",
    "path='data/'\n",
    "X,y=load_local_data(path,dataset_n, attributes=False, use_node_deg=False)\n",
    "sub2vec = Sub2vec(property='s', walkLength=1000, output='aids_walk', d=128, iter=100, windowSize=2, p=0.5, model='dm')\n",
    "# the parameters chosen are the ones from the paper, I only modified the walklenght for a smalled dataset\n",
    "sub2vec.obtainRandomWalks(X)\n",
    "embeddings = sub2vec.calculateEmbeddings()\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-calculate the embeddings and demonstrate that the cosine similarity doesn't work within rounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vects  2000\n"
     ]
    }
   ],
   "source": [
    "sub2vec.obtainRandomWalks(X)\n",
    "embeddings2 = sub2vec.calculateEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two graphs within rounds is [[0.06005514]]\n",
      "Similarity of two graphs within rounds is [[0.13349625]]\n",
      "Similarity of two graphs within rounds is [[0.09431593]]\n",
      "Similarity of two graphs within rounds is [[0.05708874]]\n",
      "Similarity of two graphs within rounds is [[-0.0155139]]\n",
      "Similarity of two graphs within rounds is [[0.17029813]]\n",
      "Similarity of two graphs within rounds is [[0.12372799]]\n",
      "Similarity of two graphs within rounds is [[-0.03390585]]\n",
      "Similarity of two graphs within rounds is [[0.16328917]]\n",
      "Similarity of two graphs within rounds is [[0.24532312]]\n"
     ]
    }
   ],
   "source": [
    "# display cosine similarity for first 10 embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "num_graphs, d = embeddings.shape\n",
    "random_graphs =  np.random.randint(0, num_graphs, size=(10))\n",
    "for i in random_graphs:\n",
    "    print(f\"Similarity of two graphs within rounds is {cosine_similarity(embeddings[i,:].reshape(1, -1), embeddings2[i,:].reshape(1, -1))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate all the values and display stats\n",
    "def meanstd_similarity(emb1, emb2):\n",
    "    res_similarity = []\n",
    "    num_graphs, d = emb1.shape\n",
    "    for i in range(num_graphs):\n",
    "        res_similarity.append(cosine_similarity(emb1[i,:].reshape(1, -1), emb2[i,:].reshape(1, -1))[0][0])\n",
    "    print(f\"Mean cosine similarity is {np.mean(res_similarity)}, std is {np.std(res_similarity)} for the pairwise comparison if {num_graphs} graphs\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity is 0.11877530068159103, std is 0.15287145972251892 for the pairwise comparison if 2000 graphs\n"
     ]
    }
   ],
   "source": [
    "meanstd_similarity(emb1 = embeddings, emb2 = embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now calculate the embeddings in one go and check whether they are similar or not this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vects  4000\n"
     ]
    }
   ],
   "source": [
    "X_double = np.hstack((X,X))\n",
    "sub2vec.obtainRandomWalks(X_double)\n",
    "embeddings1_2 = sub2vec.calculateEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two graphs from the same round is [[0.9857781]]\n",
      "Similarity of two graphs from the same round is [[0.98320913]]\n",
      "Similarity of two graphs from the same round is [[0.9268111]]\n",
      "Similarity of two graphs from the same round is [[0.95995694]]\n",
      "Similarity of two graphs from the same round is [[0.9697628]]\n",
      "Similarity of two graphs from the same round is [[0.8995649]]\n",
      "Similarity of two graphs from the same round is [[0.93944114]]\n",
      "Similarity of two graphs from the same round is [[0.9864575]]\n",
      "Similarity of two graphs from the same round is [[0.94997174]]\n",
      "Similarity of two graphs from the same round is [[0.9463763]]\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = embeddings1_2[:2000,:]\n",
    "embeddings2 = embeddings1_2[2000:,:]\n",
    "num_graphs, d = embeddings1.shape\n",
    "random_graphs =  np.random.randint(0, num_graphs, size=(10))\n",
    "for i in random_graphs:\n",
    "    print(f\"Similarity of two graphs from the same round is {cosine_similarity(embeddings1[i,:].reshape(1, -1), embeddings2[i,:].reshape(1, -1))}\")\n",
    "# much better but still very small at times, not consistent and these are the exactly same graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity is 0.9380630850791931, std is 0.10485057532787323 for the pairwise comparison if 2000 graphs\n"
     ]
    }
   ],
   "source": [
    "meanstd_similarity(emb1 = embeddings1, emb2 = embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I modified the graphs removing one single (always connected only to one or max two neighbors node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def remove_single_node(graphs_list):\n",
    "    ''' function just removes a random node from a graph which has one or two neighbors'''\n",
    "    modified_graphs = []\n",
    "    for graph in graphs_list:\n",
    "        nx_graph = graph.nx_graph # get the graph\n",
    "        degree = nx_graph.degree\n",
    "        degreeDict = dict(degree)\n",
    "        random_nodes_to_delete = []\n",
    "        # pick nodes which have 1 neighbor only\n",
    "        for node, degree in degreeDict.items():\n",
    "            if degree == 1:\n",
    "                random_nodes_to_delete.append(node)\n",
    "        # else pick nodes which have 2 neighbors\n",
    "        if len(random_nodes_to_delete) == 0:\n",
    "            for node, degree in degreeDict.items():\n",
    "                if degree == 2:\n",
    "                    random_nodes_to_delete.append(node)\n",
    "        if len(random_nodes_to_delete)==0:\n",
    "            modified_graphs.append(graph) # leave the graph as it was\n",
    "            print('A graph which has no nodes with degrees 1 & 2 detected')\n",
    "        else:\n",
    "            copy_graph = deepcopy(graph) # create a new deep copy\n",
    "            copy_graph.nx_graph.remove_node(random.choice(random_nodes_to_delete))\n",
    "\n",
    "            modified_graphs.append(copy_graph)\n",
    "        #\n",
    "    return modified_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "graph before modif has 47 and after modif 46\n",
      "graph before modif has 11 and after modif 10\n",
      "graph before modif has 9 and after modif 8\n",
      "graph before modif has 10 and after modif 9\n",
      "graph before modif has 16 and after modif 15\n",
      "graph before modif has 9 and after modif 8\n",
      "graph before modif has 11 and after modif 10\n",
      "graph before modif has 10 and after modif 9\n",
      "graph before modif has 17 and after modif 16\n",
      "graph before modif has 9 and after modif 8\n"
     ]
    }
   ],
   "source": [
    "# a small unit test of a function above\n",
    "subset_X = X[:10]\n",
    "modified_subset = remove_single_node(subset_X)\n",
    "print('done')\n",
    "for i in range(10):\n",
    "    print(f\"graph before modif has {len(subset_X[i].nx_graph)} and after modif {len(modified_subset[i].nx_graph)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now check if the resulting embeddings are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vects  4000\n"
     ]
    }
   ],
   "source": [
    "X_modified =  remove_single_node(X)\n",
    "X_original_and_modidied = np.hstack((X,X_modified))\n",
    "sub2vec.obtainRandomWalks(X_original_and_modidied)\n",
    "embeddings1_2 = sub2vec.calculateEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two graphs with a single node removed is [[0.45863032]]\n",
      "Similarity of two graphs with a single node removed is [[0.86961746]]\n",
      "Similarity of two graphs with a single node removed is [[0.9778628]]\n",
      "Similarity of two graphs with a single node removed is [[0.8524918]]\n",
      "Similarity of two graphs with a single node removed is [[0.9028555]]\n",
      "Similarity of two graphs with a single node removed is [[0.97142464]]\n",
      "Similarity of two graphs with a single node removed is [[0.92181873]]\n",
      "Similarity of two graphs with a single node removed is [[0.9858958]]\n",
      "Similarity of two graphs with a single node removed is [[0.8797715]]\n",
      "Similarity of two graphs with a single node removed is [[0.9812438]]\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = embeddings1_2[:2000,:]\n",
    "embeddings2 = embeddings1_2[2000:,:]\n",
    "num_graphs, d = embeddings1.shape\n",
    "random_graphs =  np.random.randint(0, num_graphs, size=(10))\n",
    "for i in random_graphs:\n",
    "    print(f\"Similarity of two graphs with a single node removed is {cosine_similarity(embeddings1[i,:].reshape(1, -1), embeddings2[i,:].reshape(1, -1))}\")\n",
    "# sometimes the change is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity is 0.8966434597969055, std is 0.1476893275976181 for the pairwise comparison if 2000 graphs\n"
     ]
    }
   ],
   "source": [
    "meanstd_similarity(emb1 = embeddings1, emb2 = embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Sub2Vec\n",
    "The results are not too bad, however, for some cases it is still a lot for the case when we need to compare the vectors. A complete disaster between runs - the embeddings are not coherent at all. Affected quite a lot by the random walk. \n",
    "And the main problem with this method - we cannot really use the attributes which are [potentially] extremely important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph2Vec demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method representing entire graphs as fixed length feature vectors - inspired by  neural document embedding models, the authors extend the same to learn graph embeddings.graph2vec proposes to view an entire graph as a document and the rooted subgraphs around every node in the graph as words that compose the document and extend document embedding neural networks to learn representations of entire graphs. An input of algorithms is a set of labeled or unlabeled graphs, in the case of unlabeled graphs  nodes are labeled  with their degree. In graph2vec,the are graphs analogical to documents\n",
    "that are composed of rooted subgraphs which, in turn, are\n",
    "analogical words from a special language and extend document embedding models to learn graph embeddings. The main motivation behind this idea is that structurally similar graphs will be close to each other in the embedding space. \n",
    "\n",
    "Algorithm: \n",
    "\n",
    "input -   Set of graphs\n",
    "\n",
    "1) extract rooted subgraphs and assign a unique\n",
    "label for all the rooted subgraphs in the vocabulary \n",
    "\n",
    "2) these hashed features (tags ( Weisfeiler-Lehman kernel decomposes a graph into subtrees ->https://www.cse.wustl.edu/~muhan/papers/KDD_2017.pdf) and node degrees (words) describe each sub-graph \n",
    "\n",
    "3) train a doc2vec skip-gram model using negative sampling. DBOW model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from benchmarks.graph2vec import feature_extractor # I modified the original code by the authors to directly use it on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = 'aids'\n",
    "path = 'data/'\n",
    "X, y = load_local_data(path, dataset_n, attributes=False, use_node_deg=False)\n",
    "X = list(X)\n",
    "print(\"\\nFeature extraction started.\\n\")\n",
    "document_collections = Parallel(n_jobs=1)(delayed(feature_extractor)(g,2, str(i)) for i, g in enumerate(X))\n",
    "print(\"\\nOptimization started.\\n\")\n",
    "\n",
    "model = Doc2Vec(document_collections, vector_size = 128, window = 2, min_count = 5, dm = 0,\n",
    "                sample = 0.0001, workers = 10, epochs = 20, alpha =0.025)\n",
    "embeddings = model.docvecs.vectors_docs\n",
    "print(f\"Returned embeddings shape is {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_collections2 = Parallel(n_jobs=1)(delayed(feature_extractor)(g,2, str(i)) for i, g in enumerate(X))\n",
    "model = Doc2Vec(document_collections2, vector_size = 128, window = 2, min_count = 5, dm = 0,\n",
    "                sample = 0.0001, workers = 10, epochs = 20, alpha =0.025)\n",
    "embeddings2 = model.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the similarity within the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two graphs within rounds is [[0.848712]]\n",
      "Similarity of two graphs within rounds is [[0.8972695]]\n",
      "Similarity of two graphs within rounds is [[0.8492506]]\n",
      "Similarity of two graphs within rounds is [[0.8924836]]\n",
      "Similarity of two graphs within rounds is [[0.77217966]]\n",
      "Similarity of two graphs within rounds is [[0.7804389]]\n",
      "Similarity of two graphs within rounds is [[0.87400985]]\n",
      "Similarity of two graphs within rounds is [[0.65968746]]\n",
      "Similarity of two graphs within rounds is [[0.8078866]]\n",
      "Similarity of two graphs within rounds is [[0.8581012]]\n"
     ]
    }
   ],
   "source": [
    "num_graphs, d = embeddings.shape\n",
    "random_graphs =  np.random.randint(0, num_graphs, size=(10))\n",
    "for i in random_graphs:\n",
    "    print(f\"Similarity of two graphs within rounds is {cosine_similarity(embeddings[i,:].reshape(1, -1), embeddings2[i,:].reshape(1, -1))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity is 0.8272595405578613, std is 0.05931011587381363 for the pairwise comparison if 2000 graphs\n"
     ]
    }
   ],
   "source": [
    "meanstd_similarity(emb1 = embeddings, emb2 = embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity in a  one run within the same graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_double = np.hstack((X,X))\n",
    "document_collections = Parallel(n_jobs=1)(delayed(feature_extractor)(g,2, str(i)) for i, g in enumerate(X_double))\n",
    "model = Doc2Vec(document_collections, vector_size = 128, window = 2, min_count = 5, dm = 0,\n",
    "                sample = 0.0001, workers = 10, epochs = 20, alpha =0.025)\n",
    "embeddings1_2 = model.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two graphs from the same round is [[0.9925771]]\n",
      "Similarity of two graphs from the same round is [[0.9945583]]\n",
      "Similarity of two graphs from the same round is [[0.9765564]]\n",
      "Similarity of two graphs from the same round is [[0.9957062]]\n",
      "Similarity of two graphs from the same round is [[0.98460644]]\n",
      "Similarity of two graphs from the same round is [[0.9907874]]\n",
      "Similarity of two graphs from the same round is [[0.98394835]]\n",
      "Similarity of two graphs from the same round is [[0.9813585]]\n",
      "Similarity of two graphs from the same round is [[0.9954908]]\n",
      "Similarity of two graphs from the same round is [[0.98716354]]\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = embeddings1_2[:2000,:]\n",
    "embeddings2 = embeddings1_2[2000:,:]\n",
    "num_graphs, d = embeddings1.shape\n",
    "random_graphs =  np.random.randint(0, num_graphs, size=(10))\n",
    "for i in random_graphs:\n",
    "    print(f\"Similarity of two graphs from the same round is {cosine_similarity(embeddings1[i,:].reshape(1, -1), embeddings2[i,:].reshape(1, -1))}\")\n",
    "# this actually looks pretty good! So the WL labeling gives pretty good results in comparison with random walks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity is 0.9899162650108337, std is 0.01579388417303562 for the pairwise comparison if 2000 graphs\n"
     ]
    }
   ],
   "source": [
    "meanstd_similarity(emb1 = embeddings1, emb2 = embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check if the algo is robust for node removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_modified =  remove_single_node(X)\n",
    "X_original_and_modidied = np.hstack((X,X_modified))\n",
    "document_collections = Parallel(n_jobs=1)(delayed(feature_extractor)(g,2, str(i)) for i, g in enumerate(X_original_and_modidied))\n",
    "model = Doc2Vec(document_collections, vector_size = 128, window = 2, min_count = 5, dm = 0,\n",
    "                sample = 0.0001, workers = 10, epochs = 20, alpha =0.025)\n",
    "embeddings1_2 = model.docvecs.vectors_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of two graphs from the same round is [[0.6321049]]\n",
      "Similarity of two graphs from the same round is [[0.8713942]]\n",
      "Similarity of two graphs from the same round is [[0.932484]]\n",
      "Similarity of two graphs from the same round is [[0.8647433]]\n",
      "Similarity of two graphs from the same round is [[0.7991464]]\n",
      "Similarity of two graphs from the same round is [[0.72401214]]\n",
      "Similarity of two graphs from the same round is [[0.6373059]]\n",
      "Similarity of two graphs from the same round is [[0.9049396]]\n",
      "Similarity of two graphs from the same round is [[0.84791726]]\n",
      "Similarity of two graphs from the same round is [[0.8490903]]\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = embeddings1_2[:2000,:]\n",
    "embeddings2 = embeddings1_2[2000:,:]\n",
    "num_graphs, d = embeddings1.shape\n",
    "random_graphs =  np.random.randint(0, num_graphs, size=(10))\n",
    "for i in random_graphs:\n",
    "    print(f\"Similarity of two graphs from the same round is {cosine_similarity(embeddings1[i,:].reshape(1, -1), embeddings2[i,:].reshape(1, -1))}\")\n",
    "# this is no longer good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity is 0.7509998679161072, std is 0.14412258565425873 for the pairwise comparison if 2000 graphs\n"
     ]
    }
   ],
   "source": [
    "meanstd_similarity(emb1 = embeddings1, emb2 = embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really robust to this one, so this is a limitation of graph2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion on two embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be awesome to allow to use node attributes of graphs in resulting embeddings. None of this algo uses weights of the edges either. Node2vec in the walk generation. \n",
    "\n",
    "It will be nice to have constant embeddings withing algorithm run but in the same moment to make them robust for minor graph \n",
    "changes. There are methods which allow for that (for example, subgraph mathcing kernel) - but they way too long to compute for a real-case scenario.\n",
    "If we could have had an embedding having better properties than the existing ones - it can be very interesting for some applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.node2vec import node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is very similar to word2vec embedding and is a direct improuvement of DeepWalk. The nodes are embedded based on their proximity and inter-connection, so connected node results in similar embeddings. The new idea in node2vec is the new sampling strategy allowing to explore the node neighborhood.\n",
    "\n",
    "The algorithm will take a big graph as an input, where nodes have unique labels. \n",
    "Then each node will be embedded and assigned a key-word (it's ID), and we can retrieve the embedding of the node using the ID.\n",
    "\n",
    "A demo on node2vec can be found here [https://github.com/eliorc/Medium/blob/master/Nod2Vec-FIFA17-Example.ipynb], and I tried a small one on the Geo data I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this graph has 18913 nodes\n"
     ]
    }
   ],
   "source": [
    "super_graph = nx.read_gpickle('data/supergraph_moselle_vector.gpickle')\n",
    "print(f'this graph has {len(super_graph)} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-67-46e557a7146b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-46e557a7146b>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    fig, ax = plt.subplots(figsize=(20.0, 20.0))\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "colors =np.repeat('m', len(super_graph.edges))\n",
    "for i, edge in enumerate(super_graph.edges(data=True)):\n",
    "    if edge[2]['nature']==1:\n",
    "        colors[i] = 'b'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20.0, 20.0))        \n",
    "nx.draw(super_graph,node_color='#A0CBE2',edge_color = colors,width=4,edge_cmap=plt.cm.Blues,with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
